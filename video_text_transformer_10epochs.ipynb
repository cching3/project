{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cching3/project/blob/main/video_text_transformer_10epochs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# 0. Imports & config\n",
        "# ================================================================\n",
        "import os\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    roc_auc_score,\n",
        ")\n",
        "\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "\n",
        "from transformers import RobertaModel, RobertaTokenizerFast\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "import random\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "NO1ttjxa2hPt",
        "outputId": "f0c93da6-9bf0-4548-d206-9858ef23e8c4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'nn' from partially initialized module 'torch' (most likely due to a circular import) (/usr/local/lib/python3.12/dist-packages/torch/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1632986900.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m \u001b[0;31m# needs to be after the above ATen bindings so we can overwrite from Python side\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2161\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_VF\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfunctional\u001b[0m  \u001b[0;31m# usort: skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2162\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# usort: skip # noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_add_docstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'nn' from partially initialized module 'torch' (most likely due to a circular import) (/usr/local/lib/python3.12/dist-packages/torch/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pQreJOr02jwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ================================================================\n",
        "# 1. Paths & basic hyperparams\n",
        "# ================================================================\n",
        "# TODO: change these to match your env (these mirror your old notebooks)\n",
        "DATASET_ROOT = Path(\"/content/drive/MyDrive/Deep Learning Project/Real-life_Deception_Detection_2016\")\n",
        "CSV_ROOT     = Path(\"/content/drive/MyDrive/Deep Learning Project/Real-life_Deception_Detection_2016/split_data\")\n",
        "\n",
        "CLIPS_DIR        = DATASET_ROOT / \"Clips\"\n",
        "TRANSCRIPTS_ROOT = DATASET_ROOT / \"Transcription\"\n",
        "\n",
        "train_df = pd.read_csv(CSV_ROOT / \"train.csv\")\n",
        "test_df  = pd.read_csv(CSV_ROOT / \"test.csv\")\n",
        "\n",
        "video_col = train_df.columns[0]\n",
        "label_col = train_df.columns[1]\n",
        "\n",
        "print(\"Train size:\", len(train_df))\n",
        "print(\"Test size:\", len(test_df))\n",
        "\n",
        "# Video hyperparams\n",
        "IMG_SIZE       = 224\n",
        "MAX_SEQ_LENGTH = 20      # max frames per video\n",
        "BATCH_SIZE     = 4\n",
        "EPOCHS         = 10       # you can bump up later\n",
        "LR             = 2e-5\n",
        "\n",
        "# Transformer (video) hyperparams\n",
        "EMBED_DIM  = 256\n",
        "NUM_HEADS  = 4\n",
        "FF_DIM     = 512\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT    = 0.3\n",
        "\n",
        "# Text hyperparams\n",
        "TEXT_MAX_LEN = 256\n",
        "ROBERTA_NAME = \"roberta-base\"\n",
        "\n",
        "NUM_CLASSES = 2\n"
      ],
      "metadata": {
        "id": "7UVdtIjt2mGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ================================================================\n",
        "# 2. Label + transcript helpers\n",
        "# ================================================================\n",
        "def get_transcript_path_and_label(video_name, raw_label):\n",
        "    \"\"\"\n",
        "    video_name: e.g. 'Deceptive/TrialXYZ.mp4' or just 'TrialXYZ.mp4'\n",
        "    raw_label: may be 'Truthful'/'Deceptive' or 1/0\n",
        "    Returns:\n",
        "        transcript_path (Path)\n",
        "        numeric_label (int: 0=Deceptive, 1=Truthful)\n",
        "    \"\"\"\n",
        "    base = os.path.splitext(os.path.basename(video_name))[0]\n",
        "    txt_name = base + \".txt\"\n",
        "\n",
        "    lbl_str = str(raw_label).lower()\n",
        "    if lbl_str in [\"truthful\", \"truth\", \"1\", \"true\"]:\n",
        "        subdir = \"Truthful\"\n",
        "        numeric_label = 1\n",
        "    else:\n",
        "        subdir = \"Deceptive\"\n",
        "        numeric_label = 0\n",
        "\n",
        "    transcript_path = TRANSCRIPTS_ROOT / subdir / txt_name\n",
        "    return transcript_path, numeric_label\n",
        "\n",
        "\n",
        "def build_texts_and_labels_for_df(df):\n",
        "    texts   = []\n",
        "    labels  = []\n",
        "    missing = 0\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        video_name = row[video_col]\n",
        "        raw_label  = row[label_col]\n",
        "\n",
        "        transcript_path, numeric_label = get_transcript_path_and_label(video_name, raw_label)\n",
        "\n",
        "        if transcript_path.exists():\n",
        "            with open(transcript_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                text = f.read().strip()\n",
        "            texts.append(text)\n",
        "            labels.append(numeric_label)\n",
        "        else:\n",
        "            missing += 1\n",
        "\n",
        "    print(f\"Loaded {len(texts)} transcripts, missing {missing}\")\n",
        "    return np.array(texts, dtype=object), np.array(labels, dtype=np.int64)\n",
        "\n",
        "\n",
        "print(\"Building texts for TRAIN...\")\n",
        "train_texts, train_labels_y = build_texts_and_labels_for_df(train_df)\n",
        "\n",
        "print(\"Building texts for TEST...\")\n",
        "test_texts, test_labels_y = build_texts_and_labels_for_df(test_df)"
      ],
      "metadata": {
        "id": "u_5guSqy2oSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# 3. Video feature extractor (ResNet18 backbone)\n",
        "# ================================================================\n",
        "frame_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std =[0.229, 0.224, 0.225],\n",
        "    ),\n",
        "])\n",
        "\n",
        "resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "resnet_feature_extractor = nn.Sequential(*list(resnet.children())[:-2])  # up to last conv\n",
        "for p in resnet_feature_extractor.parameters():\n",
        "    p.requires_grad = False\n",
        "resnet_feature_extractor.to(device)\n",
        "resnet_feature_extractor.eval()\n",
        "\n",
        "FEAT_DIM = 512  # conv5 channels\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_frame_features(frames):\n",
        "    \"\"\"\n",
        "    frames: list of PIL images\n",
        "    returns: (T, FEAT_DIM) frame features\n",
        "    \"\"\"\n",
        "    if len(frames) == 0:\n",
        "        return torch.zeros((0, FEAT_DIM), device=device)\n",
        "\n",
        "    tensors = [frame_transform(f) for f in frames]\n",
        "    batch = torch.stack(tensors).to(device)  # (T,3,H,W)\n",
        "\n",
        "    feats = resnet_feature_extractor(batch)          # (T, 512, H', W')\n",
        "    feats = F.adaptive_avg_pool2d(feats, (1, 1))     # (T,512,1,1)\n",
        "    feats = feats.view(feats.size(0), FEAT_DIM)      # (T,512)\n",
        "    return feats\n",
        "\n",
        "\n",
        "def load_video_frames_uniform(path, max_frames=MAX_SEQ_LENGTH):\n",
        "    \"\"\"\n",
        "    Load up to max_frames frames uniformly from the video.\n",
        "    Returns list of PIL Images.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(str(path))\n",
        "    frames = []\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    if total_frames == 0:\n",
        "        cap.release()\n",
        "        return frames\n",
        "\n",
        "    # choose indices uniformly\n",
        "    idxs = np.linspace(0, total_frames - 1, num=min(max_frames, total_frames), dtype=int)\n",
        "\n",
        "    frame_id = 0\n",
        "    idxs_set = set(int(i) for i in idxs)\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if frame_id in idxs_set:\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(Image.fromarray(frame_rgb))\n",
        "        frame_id += 1\n",
        "\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "\n",
        "def prepare_video_features_for_df(df):\n",
        "    \"\"\"\n",
        "    Precompute ResNet features for each video in df.\n",
        "    Returns:\n",
        "        features: (N, MAX_SEQ_LENGTH, FEAT_DIM)\n",
        "        masks:    (N, MAX_SEQ_LENGTH) bool, True where valid frame\n",
        "        labels:   (N,) numeric labels 0/1\n",
        "    \"\"\"\n",
        "    N = len(df)\n",
        "    features = np.zeros((N, MAX_SEQ_LENGTH, FEAT_DIM), dtype=np.float32)\n",
        "    masks    = np.zeros((N, MAX_SEQ_LENGTH), dtype=bool)\n",
        "    labels   = np.zeros((N,), dtype=np.int64)\n",
        "\n",
        "    for idx, (_, row) in enumerate(df.iterrows()):\n",
        "        video_name = row[video_col]\n",
        "        raw_label  = row[label_col]\n",
        "\n",
        "        # label to numeric\n",
        "        _, numeric_label = get_transcript_path_and_label(video_name, raw_label)\n",
        "        labels[idx] = numeric_label\n",
        "\n",
        "        video_path = CLIPS_DIR / video_name\n",
        "        frames = load_video_frames_uniform(video_path, max_frames=MAX_SEQ_LENGTH)\n",
        "        with torch.no_grad():\n",
        "            feats = extract_frame_features(frames)  # (T, FEAT_DIM)\n",
        "        T = min(MAX_SEQ_LENGTH, feats.shape[0])\n",
        "        if T > 0:\n",
        "            features[idx, :T, :] = feats[:T].cpu().numpy()\n",
        "            masks[idx, :T] = True\n",
        "\n",
        "        if (idx + 1) % 5 == 0 or idx == N - 1:\n",
        "            print(f\"Processed {idx+1}/{N} videos\", end=\"\\r\")\n",
        "\n",
        "    print(\"\\nDone precomputing video features.\")\n",
        "    return features, masks, labels\n",
        "\n",
        "\n",
        "print(\"Precomputing VIDEO features for TRAIN...\")\n",
        "train_features, train_masks, train_labels_video = prepare_video_features_for_df(train_df)\n",
        "\n",
        "print(\"Precomputing VIDEO features for TEST...\")\n",
        "test_features, test_masks, test_labels_video = prepare_video_features_for_df(test_df)\n",
        "\n",
        "# Sanity check labels are consistent between text + video paths\n",
        "assert np.all(train_labels_y == train_labels_video), \"Train label mismatch!\"\n",
        "assert np.all(test_labels_y  == test_labels_video),  \"Test label mismatch!\"\n",
        "\n",
        "labels_train = train_labels_y\n",
        "labels_test  = test_labels_y\n",
        "\n",
        "print(\"Train features shape:\", train_features.shape)\n",
        "print(\"Train masks   shape:\", train_masks.shape)"
      ],
      "metadata": {
        "id": "Cyvfq7Zm_UC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# 4. Dataset for multimodal data\n",
        "# ================================================================\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(ROBERTA_NAME)\n",
        "\n",
        "class MultimodalDeceptionDataset(Dataset):\n",
        "    def __init__(self, features, masks, texts, labels, tokenizer, text_max_len):\n",
        "        \"\"\"\n",
        "        features: (N, T, FEAT_DIM)\n",
        "        masks:    (N, T) bool\n",
        "        texts:    array/list of strings length N\n",
        "        labels:   (N,) int64\n",
        "        \"\"\"\n",
        "        self.features   = features\n",
        "        self.masks      = masks\n",
        "        self.texts      = texts\n",
        "        self.labels     = labels\n",
        "        self.tokenizer  = tokenizer\n",
        "        self.text_max_len = text_max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        vid_feat = torch.tensor(self.features[idx], dtype=torch.float32)   # (T, FEAT_DIM)\n",
        "        vid_mask = torch.tensor(self.masks[idx],   dtype=torch.bool)      # (T,)\n",
        "\n",
        "        text = str(self.texts[idx])\n",
        "        enc  = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.text_max_len,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        input_ids      = enc[\"input_ids\"].squeeze(0)        # (L,)\n",
        "        attention_mask = enc[\"attention_mask\"].squeeze(0)   # (L,)\n",
        "\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "\n",
        "        return {\n",
        "            \"video_feats\": vid_feat,\n",
        "            \"video_mask\":  vid_mask,\n",
        "            \"input_ids\":   input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\":      label,\n",
        "        }\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# 5. Video Transformer encoder\n",
        "# ================================================================\n",
        "class VideoTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        feat_dim,\n",
        "        max_seq_len,\n",
        "        embed_dim=256,\n",
        "        num_heads=4,\n",
        "        ff_dim=512,\n",
        "        num_layers=2,\n",
        "        dropout=0.3,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        self.input_proj = nn.Linear(feat_dim, embed_dim)\n",
        "        self.pos_embed  = nn.Parameter(torch.zeros(1, max_seq_len, embed_dim))\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=ff_dim,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask_valid):\n",
        "        \"\"\"\n",
        "        x: (B, T, feat_dim)\n",
        "        mask_valid: (B, T) bool, True where there is a real frame\n",
        "        \"\"\"\n",
        "        B, T, _ = x.shape\n",
        "\n",
        "        x = self.input_proj(x)                         # (B,T,E)\n",
        "        x = x + self.pos_embed[:, :T, :]              # (1,T,E) broadcast\n",
        "\n",
        "        # Transformer uses src_key_padding_mask with True = PAD\n",
        "        src_key_padding_mask = ~mask_valid            # (B,T)\n",
        "\n",
        "        out = self.encoder(x, src_key_padding_mask=src_key_padding_mask)  # (B,T,E)\n",
        "\n",
        "        # Masked mean pooling over valid frames\n",
        "        mask_f = mask_valid.unsqueeze(-1).float()     # (B,T,1)\n",
        "        summed = (out * mask_f).sum(dim=1)            # (B,E)\n",
        "        denom = mask_f.sum(dim=1).clamp(min=1.0)      # (B,1)\n",
        "        pooled = summed / denom                       # (B,E)\n",
        "        pooled = self.dropout(pooled)\n",
        "        return pooled                                 # (B,E)\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# 6. Combined multimodal classifier\n",
        "# ================================================================\n",
        "class MultiModalClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        feat_dim,\n",
        "        max_seq_len,\n",
        "        num_classes=2,\n",
        "        embed_dim=EMBED_DIM,\n",
        "        num_heads=NUM_HEADS,\n",
        "        ff_dim=FF_DIM,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        dropout=DROPOUT,\n",
        "        text_model_name=ROBERTA_NAME,\n",
        "        freeze_text_encoder=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # Video encoder\n",
        "        self.video_encoder = VideoTransformer(\n",
        "            feat_dim=feat_dim,\n",
        "            max_seq_len=max_seq_len,\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            ff_dim=ff_dim,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "        # Text encoder (RoBERTa)\n",
        "        self.text_encoder = RobertaModel.from_pretrained(text_model_name)\n",
        "        text_hidden = self.text_encoder.config.hidden_size\n",
        "\n",
        "        if freeze_text_encoder:\n",
        "            for p in self.text_encoder.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        # Project text features into same space as video\n",
        "        self.text_proj = nn.Linear(text_hidden, embed_dim)\n",
        "\n",
        "        # Fusion + classifier\n",
        "        fusion_in_dim = embed_dim * 2\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(fusion_in_dim, fusion_in_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(fusion_in_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, video_feats, video_mask, input_ids, attention_mask, labels=None):\n",
        "        \"\"\"\n",
        "        video_feats: (B,T,FEAT_DIM)\n",
        "        video_mask:  (B,T) bool\n",
        "        input_ids:   (B,L)\n",
        "        attention_mask: (B,L)\n",
        "        labels: (B,) optional\n",
        "        \"\"\"\n",
        "        # Video representation\n",
        "        vid_repr = self.video_encoder(video_feats, video_mask)    # (B,E)\n",
        "\n",
        "        # Text representation (CLS token)\n",
        "        text_out = self.text_encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "        cls_repr = text_out.last_hidden_state[:, 0, :]           # (B, H_text)\n",
        "        text_repr = self.text_proj(cls_repr)                     # (B,E)\n",
        "\n",
        "        # Fuse\n",
        "        fused = torch.cat([vid_repr, text_repr], dim=1)          # (B,2E)\n",
        "        logits = self.fusion(fused)                              # (B,num_classes)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# 7. Training / evaluation helpers\n",
        "# ================================================================\n",
        "def train_one_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_examples = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        video_feats    = batch[\"video_feats\"].to(device)          # (B,T,FEAT_DIM)\n",
        "        video_mask     = batch[\"video_mask\"].to(device)           # (B,T)\n",
        "        input_ids      = batch[\"input_ids\"].to(device)            # (B,L)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)       # (B,L)\n",
        "        labels         = batch[\"labels\"].to(device)               # (B,)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits, loss = model(video_feats, video_mask, input_ids, attention_mask, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        total_correct += (preds == labels).sum().item()\n",
        "        total_examples += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total_examples\n",
        "    avg_acc  = total_correct / total_examples\n",
        "    return avg_loss, avg_acc\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    y_prob1 = []   # probability of class 1 (Truthful)\n",
        "\n",
        "    total_examples = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        video_feats    = batch[\"video_feats\"].to(device)\n",
        "        video_mask     = batch[\"video_mask\"].to(device)\n",
        "        input_ids      = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels         = batch[\"labels\"].to(device)\n",
        "\n",
        "        logits, loss = model(video_feats, video_mask, input_ids, attention_mask, labels)\n",
        "\n",
        "        probs = F.softmax(logits, dim=1)[:, 1]  # P(class=1)\n",
        "        preds = logits.argmax(dim=1)\n",
        "\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "        total_examples += labels.size(0)\n",
        "\n",
        "        y_true.extend(labels.cpu().numpy().tolist())\n",
        "        y_pred.extend(preds.cpu().numpy().tolist())\n",
        "        y_prob1.extend(probs.cpu().numpy().tolist())\n",
        "\n",
        "    avg_loss = total_loss / total_examples\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=\"binary\", pos_label=1\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        auc = roc_auc_score(y_true, y_prob1)\n",
        "    except ValueError:\n",
        "        auc = float(\"nan\")\n",
        "\n",
        "    metrics = {\n",
        "        \"loss\": avg_loss,\n",
        "        \"acc\": acc,\n",
        "        \"precision\": prec,\n",
        "        \"recall\": rec,\n",
        "        \"f1\": f1,\n",
        "        \"auc\": auc,\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# 8. 5-fold cross validation on TRAIN ONLY (with loss curves)\n",
        "# ================================================================\n",
        "N = len(labels_train)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "\n",
        "fold_results = []\n",
        "\n",
        "for fold_idx, (tr_idx, val_idx) in enumerate(skf.split(np.arange(N), labels_train), start=1):\n",
        "    print(f\"\\n========== Fold {fold_idx} / 5 ==========\")\n",
        "\n",
        "    X_train_feat = train_features[tr_idx]\n",
        "    X_train_mask = train_masks[tr_idx]\n",
        "    X_train_txt  = train_texts[tr_idx]\n",
        "    y_train      = labels_train[tr_idx]\n",
        "\n",
        "    X_val_feat   = train_features[val_idx]\n",
        "    X_val_mask   = train_masks[val_idx]\n",
        "    X_val_txt    = train_texts[val_idx]\n",
        "    y_val        = labels_train[val_idx]\n",
        "\n",
        "    train_dataset = MultimodalDeceptionDataset(\n",
        "        X_train_feat, X_train_mask, X_train_txt, y_train,\n",
        "        tokenizer=tokenizer,\n",
        "        text_max_len=TEXT_MAX_LEN,\n",
        "    )\n",
        "    val_dataset = MultimodalDeceptionDataset(\n",
        "        X_val_feat, X_val_mask, X_val_txt, y_val,\n",
        "        tokenizer=tokenizer,\n",
        "        text_max_len=TEXT_MAX_LEN,\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    model = MultiModalClassifier(\n",
        "        feat_dim=FEAT_DIM,\n",
        "        max_seq_len=MAX_SEQ_LENGTH,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        freeze_text_encoder=False,\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        filter(lambda p: p.requires_grad, model.parameters()),\n",
        "        lr=LR,\n",
        "    )\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_val_metrics = None\n",
        "    best_state_dict = None\n",
        "\n",
        "    # history for plotting this fold\n",
        "    history = {\"train_loss\": [], \"val_loss\": []}\n",
        "\n",
        "    for epoch in range(1, EPOCHS+1):\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device)\n",
        "        val_metrics = evaluate_model(model, val_loader, device)\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_metrics[\"loss\"])\n",
        "\n",
        "        print(\n",
        "            f\"Fold {fold_idx} | Epoch {epoch:02d} \"\n",
        "            f\"| train_loss={train_loss:.4f}, train_acc={train_acc:.3f} \"\n",
        "            f\"| val_loss={val_metrics['loss']:.4f}, \"\n",
        "            f\"val_acc={val_metrics['acc']:.3f}, val_f1={val_metrics['f1']:.3f}\"\n",
        "        )\n",
        "\n",
        "        if val_metrics[\"acc\"] > best_val_acc:\n",
        "            best_val_acc = val_metrics[\"acc\"]\n",
        "            best_val_metrics = val_metrics\n",
        "            best_state_dict = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "    fold_results.append({\n",
        "        \"fold\": fold_idx,\n",
        "        \"best_val_acc\": best_val_acc,\n",
        "        \"best_val_metrics\": best_val_metrics,\n",
        "        \"state_dict\": best_state_dict,\n",
        "        \"history\": history,\n",
        "    })\n",
        "    print(f\"Best Fold {fold_idx} val metrics:\", best_val_metrics)\n",
        "\n",
        "    # Plot training vs validation loss for this fold\n",
        "    epochs_range = range(1, EPOCHS + 1)\n",
        "    plt.figure()\n",
        "    plt.plot(epochs_range, history[\"train_loss\"], label=\"Train loss\")\n",
        "    plt.plot(epochs_range, history[\"val_loss\"], label=\"Val loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(f\"Fold {fold_idx} Training vs Validation Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n========== CV SUMMARY ==========\")\n",
        "for fr in fold_results:\n",
        "    print(f\"Fold {fr['fold']}: acc={fr['best_val_acc']:.3f}, \"\n",
        "          f\"f1={fr['best_val_metrics']['f1']:.3f}, \"\n",
        "          f\"auc={fr['best_val_metrics']['auc']:.3f}\")\n",
        "\n",
        "mean_acc = np.mean([fr[\"best_val_acc\"] for fr in fold_results])\n",
        "print(f\"Mean CV acc = {mean_acc:.3f}\")\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# 9. Final training on FULL train set + evaluation on HELD-OUT TEST\n",
        "#    (with a simple train-loss curve)\n",
        "# ================================================================\n",
        "print(\"\\n========== Final training on FULL train set ==========\")\n",
        "\n",
        "full_train_dataset = MultimodalDeceptionDataset(\n",
        "    train_features, train_masks, train_texts, labels_train,\n",
        "    tokenizer=tokenizer,\n",
        "    text_max_len=TEXT_MAX_LEN,\n",
        ")\n",
        "full_train_loader = DataLoader(full_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "test_dataset = MultimodalDeceptionDataset(\n",
        "    test_features, test_masks, test_texts, labels_test,\n",
        "    tokenizer=tokenizer,\n",
        "    text_max_len=TEXT_MAX_LEN,\n",
        ")\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "final_model = MultiModalClassifier(\n",
        "    feat_dim=FEAT_DIM,\n",
        "    max_seq_len=MAX_SEQ_LENGTH,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    freeze_text_encoder=False,\n",
        ").to(device)\n",
        "\n",
        "final_optimizer = torch.optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, final_model.parameters()),\n",
        "    lr=LR,\n",
        ")\n",
        "\n",
        "full_history = {\"train_loss\": []}\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    tr_loss, tr_acc = train_one_epoch(final_model, full_train_loader, final_optimizer, device)\n",
        "    full_history[\"train_loss\"].append(tr_loss)\n",
        "    print(f\"[FULL train] Epoch {epoch:02d} | loss={tr_loss:.4f}, acc={tr_acc:.3f}\")\n",
        "\n",
        "# Plot final full-train loss curve\n",
        "epochs_range = range(1, EPOCHS + 1)\n",
        "plt.figure()\n",
        "plt.plot(epochs_range, full_history[\"train_loss\"], label=\"Train loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Full-Train Loss over Epochs\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n========== Evaluate on HELD-OUT TEST set ==========\")\n",
        "test_metrics = evaluate_model(final_model, test_loader, device)\n",
        "print(\"Test metrics:\", test_metrics)"
      ],
      "metadata": {
        "id": "iE2zOVJpkilQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy (5-fold CV) = 0.729 Â± 0.058"
      ],
      "metadata": {
        "id": "V7a8fQuhIQms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "def analyze_calibration(model, dataloader, device, n_bins=10):\n",
        "    \"\"\"\n",
        "    Computes ECE and plots a Reliability Diagram for a binary classifier.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    # 1. Gather all predictions and labels\n",
        "    print(\"Gathering predictions for calibration analysis...\")\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            video_feats = batch[\"video_feats\"].to(device)\n",
        "            video_mask = batch[\"video_mask\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            # Get logits\n",
        "            logits, _ = model(video_feats, video_mask, input_ids, attention_mask, labels)\n",
        "\n",
        "            # Apply Softmax to get probabilities (confidence)\n",
        "            probs = F.softmax(logits, dim=1)[:, 1]  # Probability of Class 1 (Truthful)\n",
        "\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_probs = np.array(all_probs)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # 2. Calculate Reliability Diagram data using sklearn\n",
        "    prob_true, prob_pred = calibration_curve(all_labels, all_probs, n_bins=n_bins, strategy='uniform')\n",
        "\n",
        "    # 3. Calculate Expected Calibration Error (ECE) Manually\n",
        "    # ECE = weighted average of |accuracy - confidence| for each bin\n",
        "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
        "    bin_lowers = bin_boundaries[:-1]\n",
        "    bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    ece = 0.0\n",
        "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
        "        # Indices of samples in this bin\n",
        "        in_bin = (all_probs > bin_lower) & (all_probs <= bin_upper)\n",
        "        prop_in_bin = np.mean(in_bin)\n",
        "\n",
        "        if prop_in_bin > 0:\n",
        "            # Accuracy in this bin\n",
        "            accuracy = np.mean(all_labels[in_bin])\n",
        "            # Average confidence in this bin\n",
        "            avg_confidence = np.mean(all_probs[in_bin])\n",
        "\n",
        "            ece += np.abs(avg_confidence - accuracy) * prop_in_bin\n",
        "\n",
        "    print(f\"\\n========== Calibration Results ==========\")\n",
        "    print(f\"Expected Calibration Error (ECE): {ece:.4f}\")\n",
        "    print(f\"  * Lower is better. 0.0 is perfect calibration.\")\n",
        "    print(f\"  * < 0.05 is excellent; > 0.20 is poorly calibrated.\")\n",
        "\n",
        "    # 4. Plot Reliability Diagram\n",
        "    plt.figure(figsize=(8, 8))\n",
        "\n",
        "    # Plot perfect calibration line\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly Calibrated')\n",
        "\n",
        "    # Plot model's calibration curve\n",
        "    plt.plot(prob_pred, prob_true, marker='o', linewidth=2, label='Video-Text Model')\n",
        "\n",
        "    # Visual Polish\n",
        "    plt.title(f'Reliability Diagram\\nECE = {ece:.4f}', fontsize=14)\n",
        "    plt.xlabel('Predicted Confidence (Probability of \"Truthful\")', fontsize=12)\n",
        "    plt.ylabel('Actual Accuracy (Fraction actually \"Truthful\")', fontsize=12)\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # [Image of Reliability Diagram example]\n",
        "    # Triggering conceptual tag for understanding\n",
        "    plt.show()\n",
        "\n",
        "# --- Run the Analysis ---\n",
        "# Assumes 'final_model' and 'test_loader' are already defined from your notebook\n",
        "analyze_calibration(final_model, test_loader, device)"
      ],
      "metadata": {
        "id": "T9VCR-_nIWDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Re-gather the probabilities from the model\n",
        "# (We need to run this briefly to populate the list again)\n",
        "final_model.eval()\n",
        "all_probs = []\n",
        "\n",
        "print(\"Gathering predictions for histogram...\")\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        # Move data to the correct device\n",
        "        video_feats = batch[\"video_feats\"].to(device)\n",
        "        video_mask = batch[\"video_mask\"].to(device)\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits, _ = final_model(video_feats, video_mask, input_ids, attention_mask, labels)\n",
        "\n",
        "        # Calculate probabilities for \"Truthful\" (Class 1)\n",
        "        probs = F.softmax(logits, dim=1)[:, 1]\n",
        "        all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "# 2. Plot the Histogram\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(all_probs, bins=20, range=(0, 1), edgecolor='black', alpha=0.7, color='skyblue')\n",
        "plt.title(\"Histogram of Predicted Confidences (Truthful)\", fontsize=14)\n",
        "plt.xlabel(\"Predicted Probability\", fontsize=12)\n",
        "plt.ylabel(\"Number of Videos\", fontsize=12)\n",
        "\n",
        "# Add a red line at 0.5 (Indecision)\n",
        "plt.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label=\"Indecision Threshold\")\n",
        "plt.legend()\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_jQ_aLitJG9P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}